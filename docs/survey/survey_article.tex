\documentclass{article}
\usepackage[a4paper, top=3cm, bottom=3cm]{geometry}
\usepackage[latin1]{inputenc}
\usepackage{setspace}
\usepackage{fancyhdr}
\usepackage{tocloft}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{float}
\usepackage{multicol}
\usepackage{indentfirst}
\usepackage[numbers]{natbib}


\pdfpagewidth 8.5in
\pdfpageheight 11in
\topmargin -1in
\headheight 0in
\headsep 0in
\textheight 8.5in
\textwidth 6.5in
\oddsidemargin 0in
\evensidemargin 0in
\headheight 77pt
\headsep .5in
\footskip .75in
\setlength{\columnsep}{25pt}
\setlength{\parskip}{1ex plus 0.5ex minus 0.2ex}


\fancyhf{}
\fancyhead[C]{A Trung Huynh's PhD work - UCL, London}
\pagestyle{fancy}

%\pagenumbering{}
% Set book title
\title{\textbf{Opinion Mining Survey}}
% Include Author name and Copyright holder name
\author{trunghlt@gmail.com}\date{}

\begin{document}

\maketitle
\thispagestyle{fancy}

\begin{multicols}{2}
 
\section{Introduction}
    Even before the internet, we frequently ask our friends for recommendation
or review about stuff we are interested in. The web has enabled collecting 
experience and opinions from a vast pool of users. The demand of these 
information interpretation and summarization has been increasing for both
invididual customers and corporations.

    From the surveys \citet{comScore2007}, \citet{Horrigan2008}, 81\% of Internet 
users (60\% of Americans) have done online research on a product at least once;
20\% (15\% of all Americans) do so on a typical day; Among readers of online 
reviews of retaurants, hotels, and various services (e.g., travel agencies or 
doctors), between 73\% and 87\% report that reviews had a significant influence 
on their purchases; Consumers report being willing to pay from 20\% to 99\% more for
a 5-star-rated item than a 4-star-rated item (the variance stems from what
type of item or service is considered); 32\% have provided a rating on a product, 
service, or person via an online rating system, and 30\% (including 18\% of 
online senior citizens) have posted an online comment or review ragarding a 
product or service.

    For political information, Rainie and Horrigan \citet{Rainie2007} studied 
the 31\% of Americans - over 60 million people that were 2006 capaign internet 
users: 28\% said that a major reason for these online activities was to
get perspectives from within their community; 34\% said that a major reason was 
to get perspectives from outside their community; 27\% had looked online for 
the endorsements or ratings of external organizations.
      
    \citet{ForrestWave2006} notes 75,000 new blogs are created daily; 1.2 million 
new posts each day. Therefore businesses require a new tool to replace traditional 
methods in order to keep track of consumer opinions.

\subsection{Applications}
    \subsubsection{Applications to review-related websites}
    \begin{itemize}
      \item A review-oriented search engine
      \item An automated review and opinion-aggregation machine.
      \item Summarizing user reviews
      \item Recommendation system
    \end{itemize}
    \subsubsection{Applications as a sub-component technology}
    \begin{itemize}
      \item Suggesting relevant ads (if an article has positive polarity about
an  item/server, advertisers can put ads about the item/service, otherwise put
ads  about its competitors).
      \item Opinion-oriented questions may require different treatment.
    \end{itemize}
    \subsubsection{Applications in businesss and government intelligence}
    \begin{itemize}
      \item Reputation management and public relationship
      \item Trend prediction
    \end{itemize}
    \subsubsection{Applications across different domains}
    \begin{itemize}
      \item In politics, understanding what voters are thinking - clarification
of politicians' positions, such as what public figures support or oppose, to
enhance the quality of information that voters have accesss to.      
      \item Automatic analysis of the opinions that people submit about pending
policy or government-regulation proposals \citet{Cynthia2006}, \citet{Kwon2006}.
    \end{itemize}

\section{Challenges}
    Consider the following sentence from Mark Twain: ``Jane Austen's books
madden me so that I can't conceal my frenzy from reader''. Regular lexicon
methods can't help in this case because ``madden'' and ``frenzy'' suggests
negative sentiment, while the whole sentiment should be positive. In order to
solve the problem of sentiment analysis completely, it is necessary to consider
the whole context in an article rather than purely key words.
    
    Lets consider another example from \citet{Liu2010}:
    \begin{verbatim}
    (1) I bought an iPhone 2 days ago. 
    (2) It was such a nice phone. (3) 
    The touch screen was really cool. 
    (4) The voice quality was clear too. 
    (5) However, my mother was mad with 
    me as I did not tell her before I 
    bought it. (6) She also thought the 
    phone was too expensive, and wanted 
    me to return it to the shop...
    \end{verbatim}

    There are several different opinions here. The sentence (2) expresses
a positive sentiment about the iphone as general. The sentences (3) and
(4) also express positive opinions but about the phone's screen and voice
quality. The sentence (5) and (6) express negative opinions of the author's
mother on the author and the phone's price. As we can see, sentiment detection
is more complicating as in order to detect the opinions of the author on iphone,
in each phrase which contains opinions, we need to detect if the opinion holder
is the author himself and if he is talking about either the iphone or its
features. Therefore the problem of sentiment detection potentially contains
difficult subproblems which are entity detection, feature extraction
and subjectivity and objectivity classification of opinions.
  
  \subsection{Named Entity Recognition (NER)}
    This is a hard problem that has been investigated widely by researchers
around the world. State-of-the-art NER systems for English produce near-human
performance \citet{Wikipedia_NER}. The best systen entering MUC-7 scored 93.39\%
of f-measure while human annotator scored 97.60\% and 96.95\% \citet{Marsh1998}.
However most of NER systems work well in some specific domains and need to be
retrained for other domains.

    This problem also contains the problems of feature extraction and synonym
grouping as they are just relationships of detected entities
\cite{WebKnox2011}. This part of the problem has remained a major challenge
even though there are many attempts to solve it.

%   \section{Subjectivity and Objectivity classification}
%     Consider a simple context:
%     \begin{quote}
%       Even though it is said iPhone 4 is brittle, I still like it and
% have just bought one for my own.
%     \end{quote}
%     Then simple sentence includes two phrases: the first phrase tells a
% fact that iPhone 4 is easy to break and the second phrase expresses the
% author's
% own sentiment about the phone. Even though the first phrase implies a
% negative sentiment, it is a objective fact and should not 

  \subsection{Opinion orientation classification}
    This is the scope where this theis attempts to solve by assuming 
entity identifications in contexts are given (using available NER systems or
manual annotations). 
    
    This most popular approach to this problem is to use opinion words
such as \textit{good}, \textit{bad}, \textit{poor}, \textit{brilliant} to
predict the sentiment in the context. However this approach is brittle because
we need different sets of opinion words for different domain for it to work
efficiently. \citet{Liu2010} gives an example about a phone battery: ``The
battery is long'', and its camera: ``This camera takes a long time to focus''.
Even though the word ``long'' is used in both sentences, the former implies a
positive sentiment while the latter implies a negative sentiment.

    Another the problem with this approach is that it is not easy to
build a lexicon that works efficiently. There are unlimited number of
expressions that people use to express opinions. \citet{Pang2002} show that
there are words that we might not think they indicate any sentiment but they
appears more frequently in a certain class, i.e ``still'' surprisingly mainly
appears in posstive class.

    We also need to build different lexicons for different domains
because a bag of words in this domain is postive but can be negative in another
domain. For example ``go read the book'' most likely indicates positive
sentiment for book reviews, but negative sentiment for movie reviews.

 
%   \subsection{Sentiment summarization}

\section{The approaches}

  \subsection{Sentence/phrase-level sentiment analysis}

    \subsubsection{Lexicon based}
      
        A typical approach to sentiment analysis is to use a lexicon of positive
  and negative words and phrases to identify the overall sentiment in a sentence
  or an article. 

        \textbf{Unsupervised approaches}

            \citet{Wiebe2000}, \citet{Turney2002}, \citet{Pan2010} built
a  lexicon using some function based on positive, negative polarity or
subjectivity  with in it.

            \citet{Liu98} use combination of this method and
the sentiment polarity of the previous sentence if the scoring function does
not indicate a deterministic classification of a given sentence.

            \citet{Hatzivassiloglou1997}
use a semi-supervised method to build such a lexicon. Their idea is to
classify words by searching for their relationship by looking at conjunctions 
such as ``elegant but over-priced'', or ``clever and informative''. In
\citet{Hatzivassiloglou1997}, after clustering, they simply assign the highest
average frequency cluster as ``positive cluster''. The classificaiton
precision is more than 90\%. In some other work, some \textit{seed words} which are
known as possitive or negative are used to predict sentiment orientation of other
words in same clusters or using Wordnet-defined relations.      		
            
            \citet{Pan2010} propose a spectral feature align (\textbf{SFA}) 
algorithm to align domain specific words from different domains into unified 
clusters, with the help of domain-independent words as a bridge. Compared to 
previous approaches, \textbf{SFA} can discover a robust representation for 
cross-domain data by fully exploiting the relationship between the domain-specific
and domain-independent words via simultaneously co-clustering them in a common
latent space.
      
        \textbf{Semi-supervised approaches}

          The idea is to use the output of an initial simple classifier to feed
  labeled data into a supervised trainer. The trained models learn
  certain patterns from the input data and hence can classify a wider set of
  inputs.

            \cite{Riloff2003} use an initial classifier to learn patterns
  of subjective expressions (an interesting example is the noun ``fact'', as in
  ``The fact is...'', exhibits high correlation with subjectivity.
            
            \cite{Kaji2006} use a similar method to learn patterns of HTML
  documents with polarity labels.
            
            Similar work can be found at \cite{Wiebea2005}, \cite{Riloff2003a}.

        \textbf{Supervised approaches}
            
            However lexicon based approaches are not that trivial.
  \citet{Pang2002} built a lexicon applying machine learning techniques based on
  unigram models that can achieve over 80\% in accuracy.

            A comparison of supervised and unsupervised methods can be found in
  \cite{Chaovalit2005}.

  \subsection{Sentiment summarization}
    \subsubsection{Supervised approaches}
        \citet{Beineke2004} try to detect the summary quotation of the sentiment 
in a review from \url{www.rottentomatoes.com}. They experimented on two learning 
methods e.g. Naive Bayes and Regularized Logistic Regression to detect the summary
quotation basing on some information like the length and location of sentences,
dictionary words in the sentences and some other combinations. However the
result is not good and the correct percentages of both methods are around or
lower than 25\%. It does not take into account that the sentiment in a review is
expressed in not only one sentence.         

        \citet{Martineau2009a} present a technique named \textbf{Delta TF-IDF}
which is an intuitive general purpose technique to efficiently weight bag of word 
scores before classification. The authors then use SVM to train their models 
for and show that their method improve accuracy for three well known data sets.

        \citet{Keefe2006} introduce two new feature selection methods and three
new feature weighting methods with Naive Bayes and Support Vector Machine.
Their methods prove that it is possible to maintain a state-of-the-art
classification accuracy of 87.15\% while using less than 36\% of the features.

    \subsubsection{Classification based  on relationship information}
      \textbf{Relationsips between discourse participants}
            
        \citet{Agrawal2003} observe on manual examination of 100
responses in newsgroup that the relationship between two individuals in the
``responded-to'' network is more likely to be antagonistic. Assuming
``respond-to'' links imply disagreement, they effectively classify users into
opposite camps via graph partitioning, outperforming methods that depend solely
on the textual information within a particular document.

      \textbf{Relationships between product features}
          
        \citet{Snyder2007} utilize agreement information in a task where
one must predict ratings for multiple asects of the same item. They then
construct a linear classifier to predict whether all aspects of a product are
given the same rating, and combine this prediction with that of
individual-aspect classifers so as to minimize a certain loss function.
\cite{Snyder2007}
        
       \textbf{Sentiment analysis of conditional sentences}

        \citet{Narayanan2009} study sentiment analysis of conditional sentences.
Sentiment in conditional sentences are different from normal sentences and hard
to determine. For example, in th sentence, \textit{if your Nokia phone is not
good, buy this great Samsung phone}, the author is positive about \textit{
Samsung phone} but does not express an opinion on \textit{Nokia phone}.
Conditional sentences can start with also \textit{unless, even if, until, as long
as, assuming, supposing, in case, only if}. The authors use SVM to train data
from 5 diverse domains and achieve remarkable results.


  \subsection{Multi-class classification}
    \subsubsection{Relationships between classes}
        Standard multi-class categorization focuses on capturing the
distinct features present in each class, and ignore the fact that ``5 stars'' is
much more like ``4 stars'' than ``2 stars''. \cite{Pang2005} observe that
one-vs-all multi-class categorization scheme can outperform regression for a
three-class classification problem (positive, neutral, and negative), perhaps
due to each class exhibiting a sufficently distinct vocabulary, but for more
fine-grained classification, regression emerges as the better of the two.
        
        \cite{Pang2005} formulate rating inference as a metric label
problem (\cite{Kleinberg2002}), so that a natural notion of distance between
classes (``2 stars'' and ``3 stars'' are more similar to each other than ``1
star'' and ``4 stars'' are) is captured explicitly. More specifically, an
optimal labeling is computed that balacnes the output of classifier that
considers items in isolation with the importance of assign similar lables to
similar items.

\section{Conclusion}

\bibliographystyle{plainnat}
\bibliography{library}

\end{multicols}
\end{document}

